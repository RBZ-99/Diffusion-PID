<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DiffusionPID: Interpreting Diffusion via Partial Information Decomposition</title>
    <style>
        body {
            font-family: 'Georgia', serif;
            line-height: 1.8;
            background-color: #f9f9f9;
            color: #333;
            margin: 0;
            padding: 40px;
            max-width: 100%;
        }
        h1 {
            text-align: center;
            font-size: 2.5em;
            margin-bottom: 10px;
            color: #2c3e50;
        }
        .authors {
            text-align: center;
            font-size: 1.1em;
            color: #7f8c8d;
            margin-bottom: 40px;
        }
        .abstract {
            font-size: 1.15em;
            max-width: 800px;
            margin: 0 auto 60px auto;
            text-align: justify;
            padding-left: 20px;
            padding-right: 20px;
        }
        .abstract h2 {
            font-size: 1.6em;
            margin-bottom: 20px;
            color: #34495e;
        }
        .image-container {
            max-width: 800px;
            margin: 0 auto 50px auto;
            text-align: center;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
        }
        .caption {
            font-size: 0.9em;
            color: #7f8c8d;
            margin-top: 10px;
            text-align: center;
        }
    </style>
</head>
<body>
    <h1>DiffusionPID: Interpreting Diffusion via Partial Information Decomposition</h1>
    <div class="authors">
        <p>Shaurya Dewan*, Rushikesh Zawar*, Prakanshul Saxena*, Yingshan Chang, Andrew Luo, Yonatan Bisk</p>
    </div>
    <div class="abstract">
        <h2>Abstract</h2>
        <p>
            Text-to-image diffusion models have made significant progress in generating naturalistic images from textual inputs, and demonstrate the capacity to learn and represent complex visual-semantic relationships. While these diffusion models have achieved remarkable success, the underlying mechanisms driving their performance are not yet fully accounted for, with many unanswered questions surrounding what they learn, how they represent visual-semantic relationships, and why they sometimes fail to generalize. Our work presents Diffusion Partial Information Decomposition (DiffusionPID), a novel technique that applies information-theoretic principles to decompose the input text prompt into its elementary components, enabling a detailed examination of how individual tokens and their interactions shape the generated image. We introduce a formal approach to analyze the uniqueness, redundancy, and synergy terms by applying PID to the denoising model at both the image and pixel level. This approach enables us to characterize how individual tokens and their interactions affect the model output. We first present a fine-grained analysis of characteristics utilized by the model to uniquely localize specific concepts, we then apply our approach in bias analysis and show it can recover gender and ethnicity biases. Finally, we use our method to visually characterize word ambiguity and similarity from the model's perspective and illustrate the efficacy of our method for prompt intervention. Our results show that PID is a potent tool for evaluating and diagnosing text-to-image diffusion models.
        </p>
    </div>
    <div class="image-container">
        <img src="image-url-here.jpg" alt="Description of the image">
        <div class="caption">Figure 1: Caption describing the image.</div>
    </div>
</body>
</html>
